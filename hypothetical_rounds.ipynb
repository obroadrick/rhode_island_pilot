{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, and because we wanted to collect data from a large sample regardless of the audit's completion, we drew one round with a high stopping probability.\n",
    "However, on average fewer ballots need to be sampled when samples are drawn in multiple smaller rounds.\n",
    "In this notebook we explore how audits would have transpired for the sample we drew, if it had been drawn in smaller rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from r2b2.contest import ContestType, Contest\n",
    "from r2b2.minerva2 import Minerva2\n",
    "from r2b2.minerva import Minerva\n",
    "from r2b2.eor_bravo import EOR_BRAVO\n",
    "from r2b2.so_bravo import SO_BRAVO\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's set all the same audit parameters and contest information\n",
    "contest_name = \"\\nSchool Construction and Renovation Projects\"\n",
    "tally = {'Approve' : 2391, 'Reject' : 1414}\n",
    "risk_limit = .1\n",
    "reported_winner = max(tally, key=tally.get) \n",
    "winner_votes = tally[reported_winner]\n",
    "total_relevant = sum(tally.values())\n",
    "loser_votes = total_relevant - winner_votes\n",
    "margin = (winner_votes / total_relevant) - (loser_votes / total_relevant)\n",
    "contest_reported = Contest(total_relevant, \n",
    "                            tally, \n",
    "                            num_winners=1, \n",
    "                            reported_winners=[reported_winner],\n",
    "                            contest_type=ContestType.PLURALITY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n",
      "59\n"
     ]
    }
   ],
   "source": [
    "# let's see if we can get the selection order from the 'interim report'\n",
    "# we will test by using the interim report from Pennsylvania\n",
    "filename = 'interim-report.csv'\n",
    "df = pd.read_csv(filename)\n",
    "df.head()\n",
    "#sort = tst.sort([\"Mean\"], ascending = False)\n",
    "df = df.sort_values(by=\"Ticket Numbers\", ascending=True)\n",
    "# save dataframe as csv to send to Filip and Poorvi for cross-checking risk values\n",
    "df.to_csv('Selection-Ordered-Sample.csv', index=False) \n",
    "\n",
    "approve_so = []\n",
    "reject_so = []\n",
    "for item in df[\"Audit Result\"]: #change to name of column\n",
    "    if item == 'Approve':\n",
    "        approve_so.append(1)\n",
    "        reject_so.append(0)\n",
    "    elif item == 'Reject':\n",
    "        reject_so.append(1)\n",
    "        approve_so.append(0)\n",
    "sample = {\n",
    "    'Approve': sum(approve_so),\n",
    "    'Reject': sum(reject_so),\n",
    "    'Approve_so': approve_so,\n",
    "    'Reject_so': reject_so,\n",
    "}\n",
    "print(sample['Approve'])\n",
    "print(sample['Reject'])\n",
    "\n",
    "MAXIMUM_POSSIBLE_SAMPLE = sample['Approve'] + sample['Reject']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a function to run any hypothetical round schedule and print the results\n",
    "def hypothetical_round_schedule(sample, round_schedule):\n",
    "    # Divide the sample according to this round schedule\n",
    "    samples = []\n",
    "    for round_size in round_schedule:\n",
    "        approve_so = sample['Approve_so'][0:round_size]\n",
    "        approve = sum(approve_so)\n",
    "        reject_so = sample['Reject_so'][0:round_size]\n",
    "        reject = sum(reject_so)\n",
    "        samples.append({'Approve_so':approve_so, 'Approve':approve, 'Reject_so':reject_so, 'Reject':reject})\n",
    "    # Now create new audit objects to run this hypothetical audit with each of the BRAVOs and Minervas\n",
    "    minerva2 = ('Minerva 2.0', Minerva2(risk_limit, 1.0, contest_reported))\n",
    "    minerva = ('Minerva', Minerva(risk_limit, 1.0, contest_reported))\n",
    "    so_bravo = ('Selection-Ordered BRAVO', SO_BRAVO(risk_limit, 1.0, contest_reported))\n",
    "    eor_bravo = ('End-of-Round BRAVO', EOR_BRAVO(risk_limit, 1.0, contest_reported))\n",
    "    audits = [minerva2, minerva, so_bravo, eor_bravo]\n",
    "    # Run the hypothetical audits, printing the results\n",
    "    for audit_name, audit in audits:\n",
    "        print('{}:'.format(audit_name))\n",
    "        for i, (round_size, round_sample) in enumerate(zip(round_schedule, samples)):\n",
    "            audit.execute_round(round_size, round_sample)\n",
    "            print('Round {}: {} total, {} winner: risk {} -- stopped: {}'.format(i+1, round_size, round_sample['Approve'], round(audit.pvalue_schedule[-1], 4), audit.stopped))\n",
    "            if audit.stopped:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for getting a round schedule corresponding to a divisor\n",
    "def round_schedule(divisor):\n",
    "    marginal_round_schedule = [len(sample['Approve_so']) // divisor] * (divisor - 1)\n",
    "    marginal_round_schedule.append(len(sample['Approve_so']) - (len(sample['Approve_so']) // divisor) * (divisor - 1))\n",
    "    cumulative_round_schedule = [sum(marginal_round_schedule[0:i+1]) for i in range(len(marginal_round_schedule))]\n",
    "    return cumulative_round_schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minerva 2.0:\n",
      "Round 1: 70 total, 44 winner: risk 0.0375 -- stopped: True\n",
      "Minerva:\n",
      "Round 1: 70 total, 44 winner: risk 0.0375 -- stopped: True\n",
      "Selection-Ordered BRAVO:\n",
      "Round 1: 70 total, 44 winner: risk 0.0899 -- stopped: True\n",
      "End-of-Round BRAVO:\n",
      "Round 1: 70 total, 44 winner: risk 0.0963 -- stopped: True\n"
     ]
    }
   ],
   "source": [
    "# Let's try the function above\n",
    "hypothetical_round_schedule(sample, round_schedule(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# While Minerva 1.0 requires predetermined round sizes, Minerva 2.0 could run picking round sizes that achieve some desired stopping probability\n",
    "# Here's a similar hypothetical round schedule function but now round sizes are determined by some stopping probability\n",
    "def hypothetical_by_sprob(sample, sprob):\n",
    "    # Now create new audit objects to run this hypothetical audit with each of the BRAVOs and Minervas\n",
    "    minerva2 = ('Minerva 2.0', Minerva2(risk_limit, 1.0, contest_reported))\n",
    "    minerva = ('Minerva', Minerva(risk_limit, 1.0, contest_reported))\n",
    "    so_bravo = ('Selection-Ordered BRAVO', SO_BRAVO(risk_limit, 1.0, contest_reported))\n",
    "    eor_bravo = ('End-of-Round BRAVO', EOR_BRAVO(risk_limit, 1.0, contest_reported))\n",
    "    audits = [minerva2, minerva, so_bravo, eor_bravo]\n",
    "    # Run the hypothetical audits, printing the results\n",
    "    for audit_name, audit in audits:\n",
    "        print('\\n{}:'.format(audit_name))\n",
    "        round_num = 0\n",
    "        while True:\n",
    "            round_num += 1\n",
    "            # Determine the next round size\n",
    "            if audit_name == 'Minerva' and round_num > 1:\n",
    "                round_size = int(round_size + round_size * 1.5)\n",
    "            else:\n",
    "                round_size = audit.next_sample_size(sprob)\n",
    "            if round_size > MAXIMUM_POSSIBLE_SAMPLE:\n",
    "                print('Next round size would be {} which exceeds the sample we drew of {}'.format(round_size, MAXIMUM_POSSIBLE_SAMPLE))\n",
    "                break\n",
    "\n",
    "            # Get the sample for this round size\n",
    "            approve_so = sample['Approve_so'][0:round_size]\n",
    "            approve = sum(approve_so)\n",
    "            reject_so = sample['Reject_so'][0:round_size]\n",
    "            reject = sum(reject_so)\n",
    "            cur_sample = {'Approve_so':approve_so, 'Approve':approve, 'Reject_so':reject_so, 'Reject':reject}\n",
    "\n",
    "            # Execute the round\n",
    "            audit.execute_round(round_size, cur_sample)\n",
    "            print('Round {}: {} total, {} winner: risk {} -- stopped: {}'.format(round_num, round_size, cur_sample['Approve'], round(audit.pvalue_schedule[-1], 4), audit.stopped))\n",
    "            if audit.stopped:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Minerva 2.0:\n",
      "Round 1: 44 total, 28 winner: risk 0.0918 -- stopped: True\n",
      "\n",
      "Minerva:\n",
      "Round 1: 44 total, 28 winner: risk 0.0918 -- stopped: True\n",
      "\n",
      "Selection-Ordered BRAVO:\n",
      "Round 1: 52 total, 30 winner: risk 0.7205 -- stopped: False\n",
      "Round 2: 96 total, 59 winner: risk 0.0539 -- stopped: True\n",
      "\n",
      "End-of-Round BRAVO:\n",
      "Round 1: 70 total, 44 winner: risk 0.0963 -- stopped: True\n"
     ]
    }
   ],
   "source": [
    "# Try out this new function\n",
    "hypothetical_by_sprob(sample, .5)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e774977668b7c0ae8309835a5187aa7fbf7669e7d0bb59755bc63e573643edcd"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
